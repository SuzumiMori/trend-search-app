import streamlit as st
import pandas as pd
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢ï¼ˆè‡ªå‹•å±•é–‹ï¼‰", page_icon="ğŸ–±ï¸", layout="wide")

st.title("ğŸ–±ï¸ ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆã€Œè‡ªå‹•å±•é–‹ã€æŠ½å‡ºã‚¢ãƒ—ãƒª")
st.markdown("""
ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’è‡ªå‹•ã§é€£æ‰“ã—ã€éš ã‚Œã¦ã„ã‚‹è¨˜äº‹ã‚’å…¨ã¦å±•é–‹ã—ã¦ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚
â€»Seleniumã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€å‡¦ç†ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚
""")

# --- Seleniumè¨­å®šé–¢æ•° ---
def get_driver():
    """Streamlit Cloudç­‰ã§å‹•ä½œã™ã‚‹ãŸã‚ã®ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®š"""
    options = Options()
    options.add_argument("--headless")  # ç”»é¢ã‚’è¡¨ç¤ºã—ãªã„
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã¨ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§ãƒ‰ãƒ©ã‚¤ãƒã®å‘¼ã³å‡ºã—æ–¹ãŒç•°ãªã‚‹å ´åˆã®å¸å
    driver = webdriver.Chrome(options=options)
    return driver

# --- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œé–¢æ•° ---
def scrape_with_selenium(url, max_clicks=30):
    driver = get_driver()
    extracted_data = []
    status_log = [] # ãƒ­ã‚°ç”¨

    try:
        driver.get(url)
        wait = WebDriverWait(driver, 10)
        
        # --- 1. ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€é€£æ‰“ãƒ‘ãƒ¼ãƒˆ ---
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i in range(max_clicks):
            status_text.text(f"èª­ã¿è¾¼ã¿ä¸­... ({i+1}/{max_clicks} å›ç›®ã‚¯ãƒªãƒƒã‚¯)")
            progress_bar.progress((i + 1) / max_clicks)
            
            try:
                # ãƒœã‚¿ãƒ³ã‚’æ¢ã™ (ã‚¯ãƒ©ã‚¹åã¯å‰å›ã®è­°è«–ã«åŸºã¥ã)
                more_button = wait.until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "a.js-list-article-more-button"))
                )
                
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã‚¯ãƒªãƒƒã‚¯
                driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", more_button)
                time.sleep(0.5) 
                more_button.click()
                
                # èª­ã¿è¾¼ã¿å¾…æ©Ÿ (ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…ã¤)
                time.sleep(2)
                
            except TimeoutException:
                status_log.append("ã“ã‚Œä»¥ä¸Šã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨ä»¶è¡¨ç¤ºã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                break
            except Exception as e:
                status_log.append(f"ã‚¯ãƒªãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
                # ãƒªã‚«ãƒãƒªï¼ˆå°‘ã—ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ï¼‰
                driver.execute_script("window.scrollBy(0, -100);")
                time.sleep(1)
                continue
        
        progress_bar.empty()
        status_text.text("ãƒšãƒ¼ã‚¸ã®å±•é–‹å®Œäº†ã€‚ãƒ‡ãƒ¼ã‚¿è§£æä¸­...")

        # --- 2. HTMLè§£æãƒ‘ãƒ¼ãƒˆ ---
        soup = BeautifulSoup(driver.page_source, "html.parser")
        
        # è¨˜äº‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾— (å‰å›ã®ã‚¯ãƒ©ã‚¹åã‚’ä½¿ç”¨)
        articles = soup.select("li.list-article__item")
        if not articles:
            articles = soup.select("div.list-article__item")
            
        for article in articles:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«
                title_tag = article.find("div", class_="list-article__title")
                if not title_tag: title_tag = article.find(["h3", "h4"])
                title = title_tag.get_text(strip=True) if title_tag else "ä¸æ˜"

                # URL
                link_tag = article.find("a")
                link_url = link_tag.get("href") if link_tag else ""
                if link_url and not link_url.startswith("http"):
                    # å¿…è¦ã«å¿œã˜ã¦ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’çµåˆ (ç°¡æ˜“å®Ÿè£…)
                    # link_url = "https://example.com" + link_url 
                    pass

                # æ—¥ä»˜
                date_tag = article.find("div", class_="list-article__date")
                date_text = date_tag.get_text(strip=True) if date_tag else ""

                # å ´æ‰€
                place_tag = article.find("div", class_="list-article__place")
                place_text = place_tag.get_text(strip=True) if place_tag else ""

                extracted_data.append({
                    "ã‚¤ãƒ™ãƒ³ãƒˆå": title,
                    "æ—¥ä»˜": date_text,
                    "å ´æ‰€": place_text,
                    "ãƒªãƒ³ã‚¯URL": link_url
                })
            except:
                continue

    except Exception as e:
        st.error(f"è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        driver.quit()
    
    return extracted_data, status_log

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---
with st.sidebar:
    st.header("è¨­å®š")
    target_url = st.text_input("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆURL", "https://example.com/events") # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯é©å®œå¤‰æ›´ã—ã¦ãã ã•ã„
    max_clicks = st.slider("ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€æœ€å¤§ã‚¯ãƒªãƒƒã‚¯å›æ•°", 1, 50, 30)
    
    st.info("â€»ã‚¯ãƒªãƒƒã‚¯å›æ•°ãŒå¤šã„ã»ã©æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚(30å›ã§ç´„1ã€œ2åˆ†)")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
if st.button("å–å¾—é–‹å§‹", type="primary"):
    if not target_url:
        st.error("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã¾ã™..."):
            data, logs = scrape_with_selenium(target_url, max_clicks)
        
        # ãƒ­ã‚°ã®è¡¨ç¤ºï¼ˆæŠ˜ã‚ŠãŸãŸã¿ï¼‰
        with st.expander("å®Ÿè¡Œãƒ­ã‚°ã‚’ç¢ºèª"):
            for log in logs:
                st.write(f"- {log}")
        
        if data:
            st.success(f"{len(data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸï¼")
            
            df = pd.DataFrame(data)
            
            # 1. ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
            st.dataframe(
                df,
                use_container_width=True,
                column_config={
                    "ãƒªãƒ³ã‚¯URL": st.column_config.LinkColumn("ãƒªãƒ³ã‚¯")
                }
            )
            
            # 2. CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv = df.to_csv(index=False).encode('utf-8_sig')
            st.download_button(
                label="ğŸ“¥ CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv,
                file_name="selenium_events.csv",
                mime='text/csv'
            )
        else:
            st.warning("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚HTMLã‚¯ãƒ©ã‚¹åãŒåˆã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
