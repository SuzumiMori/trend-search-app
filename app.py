import streamlit as st
import datetime
from google import genai
from google.genai import types
import os
import json
import pandas as pd
import re
import pydeck as pdk
import urllib.parse
import time

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢", page_icon="ğŸ—ºï¸")

st.title("ğŸ—ºï¸ ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆMapæ¤œç´¢")
st.markdown("ä¸»è¦ãƒ¡ãƒ‡ã‚£ã‚¢ã®ã€Œãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã€ã€Œã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ãƒšãƒ¼ã‚¸ã€ã®ã¿ã‚’å¯¾è±¡ã«æ¤œç´¢ã—ã¾ã™ã€‚")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    st.markdown("### ğŸ“ åœ°åŸŸãƒ»å ´æ‰€")
    region = st.text_input("æ¤œç´¢ã—ãŸã„å ´æ‰€", value="æ±äº¬éƒ½æ¸‹è°·åŒº", help="å…·ä½“çš„ãªåœ°åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    st.markdown("---")
    st.markdown("### ğŸŒ æ¤œç´¢å¯¾è±¡ã‚½ãƒ¼ã‚¹")
    
    # â˜…ã“ã“ãŒé‡è¦: ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã¯ãªãã€Œãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®ãƒ‘ã‚¹ã€ã‚’æŒ‡å®š
    # site:ã‚³ãƒãƒ³ãƒ‰ã§ã“ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ç”¨èªé›†(/words/)ã‚„æ–½è¨­æƒ…å ±(/spot/)ã¯ãƒ’ãƒƒãƒˆã—ãªããªã‚Šã¾ã™
    SITE_PATHS = {
        "Fashion Press (ãƒ‹ãƒ¥ãƒ¼ã‚¹)": "fashion-press.net/news/",
        "Walkerplus (ã‚¤ãƒ™ãƒ³ãƒˆè¨˜äº‹)": "walkerplus.com/article/",
        "Walkerplus (ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒˆ)": "walkerplus.com/event_list/",
        "Let's Enjoy Tokyo (ã‚¤ãƒ™ãƒ³ãƒˆ)": "enjoytokyo.jp/event/",
        "TimeOut Tokyo (ã‚¬ã‚¤ãƒ‰)": "timeout.jp/tokyo/ja/things-to-do/",
        "PR TIMES (ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹)": "prtimes.jp/main/html/rd/p/",
        "FASHIONSNAP (ãƒ‹ãƒ¥ãƒ¼ã‚¹)": "fashionsnap.com/article/"
    }
    
    selected_labels = st.multiselect(
        "æ¤œç´¢å¯¾è±¡ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
        options=list(SITE_PATHS.keys()),
        default=["Fashion Press (ãƒ‹ãƒ¥ãƒ¼ã‚¹)", "Walkerplus (ã‚¤ãƒ™ãƒ³ãƒˆè¨˜äº‹)", "Let's Enjoy Tokyo (ã‚¤ãƒ™ãƒ³ãƒˆ)"]
    )
    
    st.info("ğŸ’¡ ç”¨èªé›†ã‚„æ–½è¨­ç´¹ä»‹ãƒšãƒ¼ã‚¸ã‚’é™¤å¤–ã—ã€æœ€æ–°ã®ã€Œè¨˜äº‹ã€ã®ã¿ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

if st.button("æ¤œç´¢é–‹å§‹", type="primary"):
    # äº‹å‰ãƒã‚§ãƒƒã‚¯
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    if not selected_labels:
        st.error("âš ï¸ æ¤œç´¢å¯¾è±¡ã‚’å°‘ãªãã¨ã‚‚1ã¤é¸æŠã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    try:
        # æ¤œç´¢å‡¦ç†æº–å‚™
        client = genai.Client(api_key=api_key)
        status_text = st.empty()
        status_text.info(f"ğŸ” {region}ã®ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’åé›†ä¸­... (ãƒã‚¤ã‚ºé™¤å»ãƒ»è¨˜äº‹é™å®šæ¤œç´¢)")

        # é¸æŠã•ã‚ŒãŸãƒ‘ã‚¹ã‚’ãƒªã‚¹ãƒˆåŒ–
        target_paths = [SITE_PATHS[label] for label in selected_labels]
        
        # æ¤œç´¢ã‚¯ã‚¨ãƒªä½œæˆ
        # site:fashion-press.net/news/ ã®ã‚ˆã†ã«ãƒ‘ã‚¹ä»˜ãã§æŒ‡å®š
        site_query = " OR ".join([f"site:{path}" for path in target_paths])
        
        today = datetime.date.today()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""
        ã‚ãªãŸã¯ã€Œã‚¤ãƒ™ãƒ³ãƒˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åé›†ãƒ­ãƒœãƒƒãƒˆã€ã§ã™ã€‚
        ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ä½¿ã„ã€Googleæ¤œç´¢çµæœã«è¡¨ç¤ºã•ã‚Œã‚‹**å…·ä½“çš„ãªã‚¤ãƒ™ãƒ³ãƒˆè¨˜äº‹**ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã€‘
        ã€Œ{region} ã‚¤ãƒ™ãƒ³ãƒˆ é–‹å‚¬ä¸­ {site_query}ã€
        ã€Œ{region} æ–°è¦ã‚ªãƒ¼ãƒ—ãƒ³ æ±ºå®š {site_query}ã€
        ã€Œ{region} æœŸé–“é™å®š {site_query}ã€

        ã€åŸºæº–æ—¥ã€‘
        æœ¬æ—¥ã¯ {today} ã§ã™ã€‚çµ‚äº†æ¸ˆã¿ã®ã‚¤ãƒ™ãƒ³ãƒˆã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚

        ã€å³å®ˆãƒ«ãƒ¼ãƒ«ã€‘
        1. **ã€Œè¨˜äº‹ãƒšãƒ¼ã‚¸ã€ã®ã¿æŠ½å‡º**: 
           - Fashion Pressã®ã€Œç”¨èªé›†(/words/)ã€ã‚„ã€Œãƒ–ãƒ©ãƒ³ãƒ‰æƒ…å ±(/brand/)ã€ã¯çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚
           - æ–½è¨­ã®ã€Œå ´æ‰€ç´¹ä»‹(/spot/)ã€ãƒšãƒ¼ã‚¸ã‚‚å«ã‚ãªã„ã§ãã ã•ã„ã€‚
           - ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€ãƒ¬ãƒãƒ¼ãƒˆã€ã‚¤ãƒ™ãƒ³ãƒˆè©³ç´°ãƒšãƒ¼ã‚¸ã®ã¿å¯¾è±¡ã§ã™ã€‚
        2. **URL**: æ¤œç´¢çµæœã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹**è¨˜äº‹ã®å€‹åˆ¥URL**ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
        3. **ä»¶æ•°**: æœ€å¤§20ä»¶æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        ã€å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ï¼‰ã€‘
        [
            {{
                "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
                "place": "é–‹å‚¬å ´æ‰€",
                "date_info": "æœŸé–“(ä¾‹: 11/1ã€œ12/25)",
                "description": "æ¦‚è¦(çŸ­ãã¦OK)",
                "source_name": "ã‚µã‚¤ãƒˆå",
                "url": "è¨˜äº‹ã®URL",
                "lat": ç·¯åº¦(æ•°å€¤ãƒ»ä¸æ˜ãªã‚‰null),
                "lon": çµŒåº¦(æ•°å€¤ãƒ»ä¸æ˜ãªã‚‰null)
            }}
        ]
        """

        # æ¤œç´¢å®Ÿè¡Œé–¢æ•°
        def execute_search(model_name):
            return client.models.generate_content(
                model=model_name,
                contents=prompt,
                config=types.GenerateContentConfig(
                    tools=[types.Tool(google_search=types.GoogleSearch())],
                    response_mime_type="application/json",
                    temperature=0.0
                )
            )

        response = None
        
        # 1.5-flash-002 ã§å®Ÿè¡Œ
        try:
            response = execute_search("gemini-1.5-flash-002")
        except Exception as e:
            error_msg = str(e)
            if "404" in error_msg or "NOT_FOUND" in error_msg:
                status_text.warning("âš ï¸ ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆä¸­...")
                try:
                    time.sleep(2)
                    response = execute_search("gemini-2.0-flash-exp")
                except Exception as e2:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e2}")
                    st.stop()
            else:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.stop()

        status_text.empty()
        
        # --- JSONãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º ---
        text = response.text.replace("```json", "").replace("```", "").strip()
        data = []
        try:
            data = json.loads(text)
        except json.JSONDecodeError as e:
            try:
                if e.msg.startswith("Extra data"):
                    data = json.loads(text[:e.pos])
                else:
                    match = re.search(r'\[.*\]', text, re.DOTALL)
                    if match:
                        data = json.loads(match.group(0))
            except:
                pass
        
        # --- ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & URLç‰©ç†ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° ---
        cleaned_data = []
        for item in data:
            name = item.get('name', '')
            url = item.get('url', '')
            
            # 1. åå‰ãƒã‚§ãƒƒã‚¯
            if not name or name.lower() in ['unknown', 'ã‚¤ãƒ™ãƒ³ãƒˆ']:
                continue
            
            # 2. URLãƒã‚§ãƒƒã‚¯ (ãƒã‚¤ã‚ºURLã‚’ç‰©ç†çš„ã«å¼¾ã)
            # Fashion Pressã® words(ç”¨èªé›†), brand(ãƒ–ãƒ©ãƒ³ãƒ‰ç´¹ä»‹), collection(ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å†™çœŸ) ãªã©ã‚’é™¤å¤–
            if "fashion-press.net/words" in url: continue
            if "fashion-press.net/brand" in url: continue
            if "fashion-press.net/collections" in url: continue
            if "enjoytokyo.jp/spot" in url: continue # ãŸã ã®æ–½è¨­ç´¹ä»‹ã‚’é™¤å¤–
            
            # 3. æŒ‡å®šã—ãŸãƒ‘ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            # (æ¤œç´¢çµæœã‹ã‚‰å¤‰ãªãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLãªã©ã‚’æ‹¾ã£ãŸå ´åˆã®å¯¾ç­–)
            is_valid_path = False
            for path in target_paths:
                # æ¤œç´¢æ™‚ã¯ site:fashion-press.net/news/ ã ãŒã€URLã¯ https://... ãªã®ã§ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã§åˆ¤å®š
                clean_path = path.replace("/", "") # ç°¡æ˜“ä¸€è‡´
                if path.split('/')[0] in url: 
                    is_valid_path = True
                    break
            
            if is_valid_path:
                cleaned_data.append(item)
            
        data = cleaned_data

        if not data:
            st.warning(f"âš ï¸ æ¤œç´¢æ¡ä»¶ã«åˆã†ã‚¤ãƒ™ãƒ³ãƒˆè¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.info("åˆ¥ã®ã‚µã‚¤ãƒˆã‚’é¸æŠã™ã‚‹ã‹ã€ã‚¨ãƒªã‚¢ã‚’å¤‰ãˆã¦ã¿ã¦ãã ã•ã„ã€‚")
            st.stop()

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
        df = pd.DataFrame(data)

        # --- 1. é«˜æ©Ÿèƒ½åœ°å›³ (Voyager) ---
        st.subheader(f"ğŸ“ {region}å‘¨è¾ºã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ—")
        st.caption(f"æŠ½å‡ºä»¶æ•°: {len(data)}ä»¶")
        
        if not df.empty and 'lat' in df.columns and 'lon' in df.columns:
            map_df = df.dropna(subset=['lat', 'lon'])
            
            if not map_df.empty:
                view_state = pdk.ViewState(
                    latitude=map_df['lat'].mean(),
                    longitude=map_df['lon'].mean(),
                    zoom=13,
                    pitch=0,
                )

                layer = pdk.Layer(
                    "ScatterplotLayer",
                    map_df,
                    get_position='[lon, lat]',
                    get_color='[255, 75, 75, 160]',
                    get_radius=200,
                    pickable=True,
                )

                st.pydeck_chart(pdk.Deck(
                    map_style='https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json',
                    initial_view_state=view_state,
                    layers=[layer],
                    tooltip={
                        "html": "<b>{name}</b><br/>{place}<br/><i>{description}</i>",
                        "style": {"backgroundColor": "steelblue", "color": "white"}
                    }
                ))
                st.caption("â€»åœ°å›³ä¸Šã®èµ¤ã„ä¸¸ã«ãƒã‚¦ã‚¹ã‚’ä¹—ã›ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
                
                # CSVä½œæˆ
                export_data = []
                for _, row in map_df.iterrows():
                    gaiyou = f"ã€æœŸé–“ã€‘{row.get('date_info')}\n{row.get('description')}"
                    export_data.append({
                        "Name": row.get('name'),
                        "ä½æ‰€": row.get('place'),
                        "æ¦‚è¦": gaiyou,
                        "å…¬å¼ã‚µã‚¤ãƒˆ": row.get('url', '')
                    })
                
                export_df = pd.DataFrame(export_data)
                csv = export_df.to_csv(index=False).encode('utf-8_sig')

                st.download_button(
                    label="ğŸ“¥ Googleãƒã‚¤ãƒãƒƒãƒ—ç”¨CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"event_map_{region}.csv",
                    mime='text/csv',
                    help="ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Googleãƒã‚¤ãƒãƒƒãƒ—ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€ã€Œä½æ‰€ã€åˆ—ã‚’ç›®å°ã®å ´æ‰€ã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
                )
            else:
                 st.info("â€»ä½ç½®æƒ…å ±ãŒç‰¹å®šã§ããªã‹ã£ãŸãŸã‚ã€åœ°å›³ã«ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ãŒã€ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã«ã¯è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚")
        else:
            st.warning("åœ°å›³ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

        # --- 2. é€Ÿå ±ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ ---
        st.markdown("---")
        st.subheader("ğŸ“‹ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ä¸€è¦§")
        
        for item in data:
            url_text = "ãªã—"
            source_label = item.get('source_name', 'æ²è¼‰ã‚µã‚¤ãƒˆ')
            
            link_label = f"{source_label} ã§è¦‹ã‚‹"
            if source_label == "Googleæ¤œç´¢":
                link_label = "ğŸ” Googleã§å†æ¤œç´¢"

            if item.get('url'):
                url_text = f"[ğŸ”— {link_label}]({item.get('url')})"

            st.markdown(f"""
            - **æœŸé–“**: {item.get('date_info')}
            - **ã‚¤ãƒ™ãƒ³ãƒˆå**: {item.get('name')}
            - **å ´æ‰€**: {item.get('place')}
            - **æ¦‚è¦**: {item.get('description')}
            - **ã‚½ãƒ¼ã‚¹**: {url_text}
            """)

    except Exception as e:
        status_text.empty()
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
