import streamlit as st
import datetime
from google import genai
from google.genai import types
import os
import json
import pandas as pd
import pydeck as pdk
import requests
from bs4 import BeautifulSoup
import time
import urllib.parse
import re

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢", page_icon="ğŸ“–", layout="wide")

st.title("ğŸ“– ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã€Œå®Œå…¨æŠ½å‡ºã€ã‚¢ãƒ—ãƒª")
st.markdown("Webãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿ã€**æ‰‹æŒã¡ã®CSVã«ãªã„æ–°ã—ã„æƒ…å ±ã®ã¿**ã‚’æŠ½å‡ºã—ã¾ã™ã€‚")

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def normalize_date(text):
    """æ—¥ä»˜ã‚’ã‚¼ãƒ­åŸ‹ã‚YYYYå¹´MMæœˆDDæ—¥å½¢å¼ã«çµ±ä¸€"""
    if not text: return text
    def replace_func(match):
        return f"{match.group(1)}å¹´{match.group(2).zfill(2)}æœˆ{match.group(3).zfill(2)}æ—¥"
    text = re.sub(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', replace_func, text)
    text = re.sub(r'(\d{4})/(\d{1,2})/(\d{1,2})', lambda m: f"{m.group(1)}/{m.group(2).zfill(2)}/{m.group(3).zfill(2)}", text)
    return text

def normalize_string(text):
    """
    æ–‡å­—åˆ—æ¯”è¼ƒç”¨ã®æ­£è¦åŒ–é–¢æ•°
    """
    if not isinstance(text, str):
        return ""
    text = text.replace(" ", "").replace("ã€€", "")
    text = text.replace("ï¼ˆ", "").replace("ï¼‰", "").replace("(", "").replace(")", "")
    return text.lower()

# --- Session State ---
if 'extracted_data' not in st.session_state:
    st.session_state.extracted_data = None
if 'last_update' not in st.session_state:
    st.session_state.last_update = None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("1. èª­ã¿è¾¼ã¿å¯¾è±¡")
    
    PRESET_URLS = {
        "PRTIMES (æœ€æ–°ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹)": "https://prtimes.jp/"
    }
    
    selected_presets = st.multiselect(
        "ã‚µã‚¤ãƒˆã‚’é¸æŠ",
        options=list(PRESET_URLS.keys()),
        default=["PRTIMES (æœ€æ–°ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹)"]
    )

    st.markdown("### ğŸ”— ã‚«ã‚¹ã‚¿ãƒ URL")
    custom_urls_text = st.text_area("ãã®ä»–ã®URL (1è¡Œã«1ã¤)", height=100, help="https://www.atpress.ne.jp/ ãªã©")
    
    st.markdown("---")
    st.markdown("### 2. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿é™¤å¤– (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)")
    uploaded_file = st.file_uploader("éå»CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (é™¤å¤–ç”¨)", type="csv")
    
    existing_fingerprints = set()
    if uploaded_file is not None:
        try:
            existing_df = pd.read_csv(uploaded_file)
            count = 0
            name_col = next((col for col in existing_df.columns if 'ã‚¤ãƒ™ãƒ³ãƒˆå' in col or 'Name' in col), None)
            place_col = next((col for col in existing_df.columns if 'å ´æ‰€' in col or 'Place' in col), None)

            if name_col:
                for _, row in existing_df.iterrows():
                    n = normalize_string(row[name_col])
                    p = normalize_string(row[place_col]) if place_col else ""
                    existing_fingerprints.add((n, p))
                    count += 1
                st.success(f"ğŸ“š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ {count}ä»¶ ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
            else:
                st.error("CSVã«ã€Œã‚¤ãƒ™ãƒ³ãƒˆåã€åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            st.error(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

if st.button("ä¸€æ‹¬èª­ã¿è¾¼ã¿é–‹å§‹", type="primary"):
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    targets = []
    for label in selected_presets:
        targets.append({"url": PRESET_URLS[label], "label": label})
    
    if custom_urls_text:
        for url in custom_urls_text.split('\n'):
            url = url.strip()
            if url and url.startswith("http"):
                domain = urllib.parse.urlparse(url).netloc
                targets.append({"url": url, "label": f"ã‚«ã‚¹ã‚¿ãƒ  ({domain})"})
    
    unique_targets = {t['url']: t for t in targets}
    targets = list(unique_targets.values())

    if not targets:
        st.error("âš ï¸ URLãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    all_data = []
    client = genai.Client(api_key=api_key)
    today = datetime.date.today()
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    total_urls = len(targets)
    skipped_count_duplicate_csv = 0
    
    # --- ãƒ«ãƒ¼ãƒ—å‡¦ç† ---
    for i, target in enumerate(targets):
        url = target['url']
        label = target['label']
        
        status_text.info(f"â³ ({i+1}/{total_urls}) è§£æä¸­...: {label}")
        progress_bar.progress(i / total_urls)
        
        try:
            headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
            response = requests.get(url, headers=headers, timeout=15)
            response.encoding = response.apparent_encoding
            
            if response.status_code != 200:
                st.warning(f"âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—: {url}")
                continue

            soup = BeautifulSoup(response.text, "html.parser")
            
            # --- ã€é‡è¦ã€‘å¾¹åº•çš„ãªãƒã‚¤ã‚ºé™¤å» ---
            # è¨˜äº‹ä¸€è¦§ä»¥å¤–ã®è¦ç´ ï¼ˆãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€åºƒå‘Šï¼‰ã‚’HTMLæ§‹é€ ã‹ã‚‰å‰Šé™¤ã—ã€
            # AIã«æ¸¡ã™ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Œãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€ã®ã¿ã«çµã‚Šè¾¼ã‚€
            
            # 1. åŸºæœ¬çš„ãªä¸è¦ã‚¿ã‚°å‰Šé™¤
            for tag in soup(["script", "style", "nav", "footer", "iframe", "header", "noscript", "form", "svg"]):
                tag.decompose()
            
            # 2. ã‚¯ãƒ©ã‚¹åã‚„IDã«ã‚ˆã‚‹ä¸è¦ã‚¨ãƒªã‚¢ã®æ¨å®šå‰Šé™¤
            # (PRTIMESã‚„AtPressã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‚„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’é™¤å¤–ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¯€ç´„)
            exclude_keywords = ['sidebar', 'side-bar', 'ranking', 'recommend', 'widget', 'advertisement', 'pankuzu', 'breadcrumb']
            for tag in soup.find_all(attrs={"class": True}):
                classes = tag.get("class")
                if isinstance(classes, list):
                    classes = " ".join(classes).lower()
                if any(k in classes for k in exclude_keywords):
                    tag.decompose()
            
            for tag in soup.find_all(attrs={"id": True}):
                ids = tag.get("id").lower()
                if any(k in ids for k in exclude_keywords):
                    tag.decompose()

            # --- ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º ---
            # separator="\n" ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥ä»˜ãªã©ãŒãã£ã¤ãã®ã‚’é˜²ã
            page_text = soup.get_text(separator="\n", strip=True)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«åˆã‚ã›ã¦åˆ‡ã‚Šå‡ºã—ï¼ˆGemini 2.0 Flashã¯100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³å¯¾å¿œãªã®ã§å¤§ãã‚ã«ï¼‰
            # ãŸã ã—ã€é•·ã™ãã‚‹ã¨å‡ºåŠ›ç”Ÿæˆï¼ˆOutput Tokenï¼‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹ãŸã‚ã€æ–‡å­—æ•°ã§ä¸€å®šã®åˆ¶é™ã¯ã‹ã‘ã‚‹
            # æ—¥æœ¬èªã§40ä¸‡æ–‡å­—ã‚ã‚Œã°ã€é€šå¸¸ã®ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã¯ååˆ†å…¥ã‚‹
            page_text = page_text[:400000]

            prompt = f"""
            ã‚ãªãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®Webãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ã€ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã‚„ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã®ã€Œä¸€è¦§ãƒªã‚¹ãƒˆã€ã§ã™ã€‚
            ã“ã®ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹**å…¨ã¦ã®è¨˜äº‹æƒ…å ±**ã‚’æŠ½å‡ºã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

            ã€é‡è¦æŒ‡ä»¤ã€‘
            1. **ã€Œè¦ç´„ã€ã‚„ã€ŒæŠœç²‹ã€ã¯ç¦æ­¢ã§ã™ã€‚ãƒªã‚¹ãƒˆã«ã‚ã‚‹é …ç›®ã¯ã€ä¸Šã‹ã‚‰ä¸‹ã¾ã§å…¨ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚**
            2. å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ï¼ˆæ–‡å­—æ•°åˆ¶é™ï¼‰ãŒè¨±ã™é™ã‚Šã€å¯èƒ½ãªé™ã‚Šå¤šãã®é …ç›®ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            3. PRTIMESã‚„AtPressã®ã‚ˆã†ãªã‚µã‚¤ãƒˆã®å ´åˆã€1ãƒšãƒ¼ã‚¸ã«50ä»¶ä»¥ä¸Šã®æƒ…å ±ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œã‚‰ã‚’ç¶²ç¾…ã—ã¦ãã ã•ã„ã€‚

            ã€å‰ææƒ…å ±ã€‘
            ãƒ»æœ¬æ—¥ã®æ—¥ä»˜: {today.strftime('%Yå¹´%mæœˆ%dæ—¥')}
            ãƒ»å‚ç…§URL: {url}
            
            ã€ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã€‘
            {page_text}

            ã€æŠ½å‡ºé …ç›®ã¨ãƒ«ãƒ¼ãƒ«ã€‘
            ãƒ»name: ã‚¤ãƒ™ãƒ³ãƒˆåã€ã¾ãŸã¯è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            ãƒ»place: é–‹å‚¬å ´æ‰€ï¼ˆè¨˜è¿°ãŒãªã‘ã‚Œã°ã€ã‚¿ã‚¤ãƒˆãƒ«ç­‰ã‹ã‚‰éƒ½é“åºœçœŒã‚„æ–½è¨­åã‚’æ¨æ¸¬ã€‚ä¸æ˜ãªã‚‰ç©ºæ¬„ï¼‰
            ãƒ»date_info: æ—¥ä»˜ï¼ˆYYYYå¹´MMæœˆDDæ—¥å½¢å¼ï¼‰
            ãƒ»description: æ¦‚è¦ï¼ˆ1è¡Œç¨‹åº¦ï¼‰
            ãƒ»lat: ç·¯åº¦(æ•°å€¤ãƒ»æ¨æ¸¬)
            ãƒ»lon: çµŒåº¦(æ•°å€¤ãƒ»æ¨æ¸¬)

            ã€å‡ºåŠ›å½¢å¼ (JSON List)ã€‘
            [
                {{ "name": "...", "place": "...", "date_info": "...", "description": "...", "lat": 0.0, "lon": 0.0 }},
                ...
            ]
            """

            # ãƒ¢ãƒ‡ãƒ«è¨­å®š: response_mime_typeã§JSONã‚’å¼·åˆ¶
            ai_response = client.models.generate_content(
                model="gemini-2.0-flash-exp",
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json", 
                    temperature=0.0
                )
            )
            
            raw_json = ai_response.text.replace("```json", "").replace("```", "").strip()
            extracted_list = json.loads(raw_json)
            
            if isinstance(extracted_list, list):
                for item in extracted_list:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯å‡¦ç†
                    n_key = normalize_string(item.get('name', ''))
                    p_key = normalize_string(item.get('place', ''))
                    
                    is_in_csv = False
                    if (n_key, p_key) in existing_fingerprints:
                        is_in_csv = True
                    elif p_key == "" and any(ef[0] == n_key for ef in existing_fingerprints):
                        is_in_csv = True
                    
                    if is_in_csv:
                        skipped_count_duplicate_csv += 1
                        continue

                    # æ¡ç”¨
                    item['source_label'] = label
                    item['source_url'] = url
                    if item.get('date_info'):
                        item['date_info'] = normalize_date(item['date_info'])
                    all_data.append(item)
            
            time.sleep(1)

        except Exception as e:
            st.warning(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {label} (ã‚¨ãƒ©ãƒ¼: {e})")
            continue

    progress_bar.progress(100)
    time.sleep(0.5)
    progress_bar.empty()

    if not all_data and skipped_count_duplicate_csv > 0:
        st.warning(f"å–å¾—ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã¦CSVã«å«ã¾ã‚Œã¦ã„ã¾ã—ãŸï¼ˆé™¤å¤–æ•°: {skipped_count_duplicate_csv}ä»¶ï¼‰ã€‚")
        st.session_state.extracted_data = None
    elif not all_data:
        st.error("æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚Webã‚µã‚¤ãƒˆã®æ§‹é€ ãŒå¤‰ã‚ã£ãŸã‹ã€ã‚¢ã‚¯ã‚»ã‚¹ãŒåˆ¶é™ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        st.session_state.extracted_data = None
    else:
        # é‡è¤‡æ’é™¤
        unique_data = []
        seen_keys = set()
        for item in all_data:
            name_key = normalize_string(item.get('name', ''))
            place_key = normalize_string(item.get('place', ''))
            if not name_key: continue
            
            if (name_key, place_key) not in seen_keys:
                seen_keys.add((name_key, place_key))
                unique_data.append(item)
        
        st.session_state.extracted_data = unique_data
        st.session_state.last_update = datetime.datetime.now().strftime("%H:%M:%S")
        
        msg = f"ğŸ‰ èª­ã¿è¾¼ã¿å®Œäº†ï¼ æ–°è¦ {len(unique_data)} ä»¶"
        if skipped_count_duplicate_csv > 0:
            msg += f" (CSVé‡è¤‡é™¤å¤–: {skipped_count_duplicate_csv} ä»¶)"
        status_text.success(msg)

# --- çµæœè¡¨ç¤ºã‚¨ãƒªã‚¢ ---

if st.session_state.extracted_data is not None:
    data = st.session_state.extracted_data
    df = pd.DataFrame(data)

    st.markdown(f"**æœ€çµ‚æ›´æ–°: {st.session_state.last_update}** ({len(data)}ä»¶)")

    # 1. ãƒãƒƒãƒ—è¡¨ç¤º
    st.subheader("ğŸ“ ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ— (æ–°è¦ã®ã¿)")
    if not df.empty and 'lat' in df.columns and 'lon' in df.columns:
        map_df = df.dropna(subset=['lat', 'lon'])
        if not map_df.empty:
            view_state = pdk.ViewState(
                latitude=map_df['lat'].mean(),
                longitude=map_df['lon'].mean(),
                zoom=11,
                pitch=0,
            )
            layer = pdk.Layer(
                "ScatterplotLayer",
                map_df,
                get_position='[lon, lat]',
                get_color='[255, 75, 75, 160]',
                get_radius=300,
                pickable=True,
            )
            st.pydeck_chart(pdk.Deck(
                map_style='https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json',
                initial_view_state=view_state,
                layers=[layer],
                tooltip={"html": "<b>{name}</b><br/>{place}<br/><i>{date_info}</i>"}
            ))

    # 2. ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
    st.markdown("---")
    st.subheader("ğŸ“‹ æ–°è¦ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§")

    display_cols = ['date_info', 'name', 'place', 'description', 'source_label', 'source_url']
    available_cols = [c for c in display_cols if c in df.columns]
    display_df = df[available_cols].copy()
    
    rename_map = {
        'date_info': 'æœŸé–“', 'name': 'ã‚¤ãƒ™ãƒ³ãƒˆå', 'place': 'å ´æ‰€', 
        'description': 'æ¦‚è¦', 'source_label': 'æƒ…å ±æº', 'source_url': 'ãƒªãƒ³ã‚¯URL'
    }
    display_df = display_df.rename(columns=rename_map)

    try:
        display_df = display_df.sort_values('æœŸé–“')
    except:
        pass

    st.dataframe(
        display_df,
        use_container_width=True,
        column_config={
            "ãƒªãƒ³ã‚¯URL": st.column_config.LinkColumn("å…ƒè¨˜äº‹", display_text="ğŸ”— ãƒªãƒ³ã‚¯ã‚’é–‹ã"),
            "æ¦‚è¦": st.column_config.TextColumn("æ¦‚è¦", width="large")
        },
        hide_index=True
    )

    # 3. CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv = display_df.to_csv(index=False).encode('utf-8_sig')
    st.download_button(
        label="ğŸ“¥ æ–°è¦åˆ†CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="events_new_only.csv",
        mime='text/csv'
    )
