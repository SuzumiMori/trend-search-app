import streamlit as st
import datetime
from google import genai
from google.genai import types
import os
import json
import pandas as pd
import re
import pydeck as pdk
import urllib.parse # URLè§£æç”¨

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆæ¤œç´¢", page_icon="ğŸ—ºï¸")

st.title("ğŸ—ºï¸ ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆMapæ¤œç´¢")
st.markdown("ä¿¡é ¼ã§ãã‚‹å¤§æ‰‹æƒ…å ±ã‚µã‚¤ãƒˆã‹ã‚‰ã€å®‰å…¨ãªãƒªãƒ³ã‚¯ã®ã¿ã‚’å³é¸ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    st.markdown("### ğŸ“ åœ°åŸŸãƒ»å ´æ‰€")
    region = st.text_input("æ¤œç´¢ã—ãŸã„å ´æ‰€", value="æ±äº¬éƒ½æ¸‹è°·åŒº", help="å…·ä½“çš„ãªåœ°åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    st.info("ğŸ’¡ ãƒªãƒ³ã‚¯åˆ‡ã‚Œã‚’é˜²ããŸã‚ã€å…¬å¼ã‚µã‚¤ãƒˆã‚„å¤§æ‰‹ãƒ¡ãƒ‡ã‚£ã‚¢ã®æ­£ã—ã„URLã®ã¿ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---

if st.button("æ¤œç´¢é–‹å§‹", type="primary"):
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
    except:
        st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    # æ¤œç´¢å‡¦ç†
    client = genai.Client(api_key=api_key)
    status_text = st.empty()
    status_text.info(f"ğŸ” {region}ã®ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’åé›†ä¸­... (URLã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ä¸­)")

    # è¨±å¯ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼‰
    # ã“ã“ã«å«ã¾ã‚Œãªã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã®URLã¯ã€Œæ€ªã—ã„ã€ã¨ã¿ãªã—ã¦å¼¾ãã¾ã™
    VALID_DOMAINS = [
        "walkerplus.com",
        "enjoytokyo.jp",
        "rurubu.jp",
        "jorudan.co.jp",
        "fashion-press.net",
        "prtimes.jp",
        "timeout.jp",
        "event-checker.info",
        "entabe.jp",
        "lmaga.jp",      # é–¢è¥¿ç³»ã«å¼·ã„
        "letsenjoytokyo.jp"
    ]

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    target_sites = " OR ".join([f"site:{d}" for d in VALID_DOMAINS])
    
    prompt = f"""
    ã‚ãªãŸã¯ã€Œã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±æŠ½å‡ºã®ãƒ—ãƒ­ã€ã§ã™ã€‚
    ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã§Googleæ¤œç´¢ã‚’è¡Œã„ã€**ç¾åœ¨é–‹å‚¬ä¸­**ã¾ãŸã¯**ä»Šå¾Œé–‹å‚¬äºˆå®š**ã®ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

    ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã€‘
    ã€Œ{region} ã‚¤ãƒ™ãƒ³ãƒˆä¸€è¦§ é–‹å‚¬ä¸­ {target_sites}ã€
    ã€Œ{region} æ–°è¦ã‚ªãƒ¼ãƒ—ãƒ³ äºˆå®š {target_sites}ã€

    ã€å³å®ˆãƒ«ãƒ¼ãƒ«ã€‘
    1. **å®Ÿåœ¨ã™ã‚‹æ­£ã—ã„URLã®ã¿æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚** `kanko.walkerplus.com` ã®ã‚ˆã†ãªæ¶ç©ºã®ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™ã€‚
    2. URLãŒä¸æ˜ç¢ºãªå ´åˆã¯ `null` ã«ã—ã¦ãã ã•ã„ã€‚
    3. ã‚¤ãƒ™ãƒ³ãƒˆåã¯æ­£ç¢ºã«æ‹¾ã£ã¦ãã ã•ã„ã€‚ã€Œunknownã€ã¯ç¦æ­¢ã§ã™ã€‚

    ã€å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ï¼‰ã€‘
    [
        {{
            "name": "ã‚¤ãƒ™ãƒ³ãƒˆå",
            "place": "é–‹å‚¬å ´æ‰€",
            "date_info": "æœŸé–“(ä¾‹: é–‹å‚¬ä¸­ã€œ12/25)",
            "description": "æ¦‚è¦",
            "source_name": "ã‚µã‚¤ãƒˆå",
            "url": "è¨˜äº‹ã®URL",
            "lat": ç·¯åº¦(æ•°å€¤),
            "lon": çµŒåº¦(æ•°å€¤)
        }}
    ]
    """

    try:
        # AIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())],
                response_mime_type="application/json",
                temperature=0.0
            )
        )

        status_text.empty()
        
        # --- JSONãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º ---
        text = response.text.replace("```json", "").replace("```", "").strip()
        data = []
        try:
            data = json.loads(text)
        except json.JSONDecodeError as e:
            try:
                if e.msg.startswith("Extra data"):
                    data = json.loads(text[:e.pos])
                else:
                    match = re.search(r'\[.*\]', text, re.DOTALL)
                    if match:
                        data = json.loads(match.group(0))
            except:
                pass
        
        # --- â˜…URLæ¤œå•ï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆãƒ»ãƒã‚§ãƒƒã‚¯ï¼‰ ---
        cleaned_data = []
        for item in data:
            name = item.get('name', '')
            url = item.get('url', '')
            
            # 1. åå‰ãƒã‚§ãƒƒã‚¯
            if not name or name.lower() in ['unknown', 'ã‚¤ãƒ™ãƒ³ãƒˆ', 'æƒ…å ±ãªã—']:
                continue
            
            # 2. URLãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯
            is_valid_url = False
            if url and url.startswith("http"):
                try:
                    domain = urllib.parse.urlparse(url).netloc
                    # ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã®ã„ãšã‚Œã‹ãŒãƒ‰ãƒ¡ã‚¤ãƒ³ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹
                    for valid_d in VALID_DOMAINS:
                        if valid_d in domain:
                            is_valid_url = True
                            break
                    
                    # ç‰¹å®šã®å¹»è¦šURLã¯åæŒ‡ã—ã§æ’é™¤
                    if "kanko.walkerplus" in url:
                        is_valid_url = False
                        
                except:
                    is_valid_url = False
            
            # URLãŒæ€ªã—ã„å ´åˆã®æ•‘æ¸ˆæªç½®
            # URLã‚’å‰Šé™¤ã™ã‚‹ã®ã§ã¯ãªãã€ã€ŒGoogleæ¤œç´¢çµæœã¸ã®ãƒªãƒ³ã‚¯ã€ã«å·®ã—æ›¿ãˆã‚‹
            if not is_valid_url:
                search_query = f"{item['name']} {item['place']} ã‚¤ãƒ™ãƒ³ãƒˆ"
                item['url'] = f"https://www.google.com/search?q={urllib.parse.quote(search_query)}"
                item['source_name'] = "Googleæ¤œç´¢" # ã‚½ãƒ¼ã‚¹åã‚‚å¤‰æ›´
            
            cleaned_data.append(item)
            
        data = cleaned_data

        if not data:
            st.warning(f"âš ï¸ ä¿¡é ¼ã§ãã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()

        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
        df = pd.DataFrame(data)

        # --- 1. é«˜æ©Ÿèƒ½åœ°å›³ (Voyager) ---
        st.subheader(f"ğŸ“ {region}å‘¨è¾ºã®ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒƒãƒ—")
        
        if not df.empty and 'lat' in df.columns and 'lon' in df.columns:
            map_df = df.dropna(subset=['lat', 'lon'])
            
            if not map_df.empty:
                view_state = pdk.ViewState(
                    latitude=map_df['lat'].mean(),
                    longitude=map_df['lon'].mean(),
                    zoom=13,
                    pitch=0,
                )

                layer = pdk.Layer(
                    "ScatterplotLayer",
                    map_df,
                    get_position='[lon, lat]',
                    get_color='[255, 75, 75, 160]',
                    get_radius=200,
                    pickable=True,
                )

                st.pydeck_chart(pdk.Deck(
                    map_style='https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json',
                    initial_view_state=view_state,
                    layers=[layer],
                    tooltip={
                        "html": "<b>{name}</b><br/>{place}<br/><i>{description}</i>",
                        "style": {"backgroundColor": "steelblue", "color": "white"}
                    }
                ))
                st.caption("â€»åœ°å›³ä¸Šã®èµ¤ã„ä¸¸ã«ãƒã‚¦ã‚¹ã‚’ä¹—ã›ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
                
                # CSVä½œæˆ
                export_data = []
                for _, row in map_df.iterrows():
                    gaiyou = f"ã€æœŸé–“ã€‘{row.get('date_info')}\n{row.get('description')}"
                    export_data.append({
                        "Name": row.get('name'),
                        "ä½æ‰€": row.get('place'),
                        "æ¦‚è¦": gaiyou,
                        "å…¬å¼ã‚µã‚¤ãƒˆ": row.get('url', '')
                    })
                
                export_df = pd.DataFrame(export_data)
                csv = export_df.to_csv(index=False).encode('utf-8_sig')

                st.download_button(
                    label="ğŸ“¥ Googleãƒã‚¤ãƒãƒƒãƒ—ç”¨CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv,
                    file_name=f"event_map_{region}.csv",
                    mime='text/csv',
                    help="ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Googleãƒã‚¤ãƒãƒƒãƒ—ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã€ã€Œä½æ‰€ã€åˆ—ã‚’ç›®å°ã®å ´æ‰€ã«æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
                )
            else:
                 st.warning("ä½ç½®æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒªã‚¹ãƒˆã®ã¿è¡¨ç¤ºã—ã¾ã™ï¼‰")

        # --- 2. é€Ÿå ±ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ ---
        st.markdown("---")
        st.subheader("ğŸ“‹ ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±ä¸€è¦§")
        st.caption("â€»ãƒªãƒ³ã‚¯å…ˆã§è©³ç´°ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
        
        for item in data:
            url_text = "ãªã—"
            source_label = item.get('source_name', 'è©³ç´°')
            
            # Googleæ¤œç´¢ãƒªãƒ³ã‚¯ã«å·®ã—æ›¿ã‚ã£ãŸå ´åˆã®è¡¨è¨˜
            link_label = f"{source_label} ã§ç¢ºèª"
            if source_label == "Googleæ¤œç´¢":
                link_label = "ğŸ” Googleã§å†æ¤œç´¢"

            if item.get('url'):
                url_text = f"[ğŸ”— {link_label}]({item.get('url')})"

            st.markdown(f"""
            - **æœŸé–“**: {item.get('date_info')}
            - **ã‚¤ãƒ™ãƒ³ãƒˆå**: {item.get('name')}
            - **å ´æ‰€**: {item.get('place')}
            - **æ¦‚è¦**: {item.get('description')}
            - **ã‚½ãƒ¼ã‚¹**: {url_text}
            """)

    except Exception as e:
        status_text.empty()
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
